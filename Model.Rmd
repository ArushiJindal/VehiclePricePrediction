---
title: "Car Price Data"
author: "Arushi"
date: "27 October 2017"
output: html_document
---

```{r}
rm(list=ls(all=TRUE))

# Loading the required libraries

library(caret)
#library(dummies)
library(DMwR)
library(infotheo)
#library(caretEnsemble)
library(xgboost)
library(ggplot2)
library(plotly)
library(gbm)
library(data.table) #To be able to change multiple columns in one go
library(mlr) #for machine learning
library(rpart) #Decision Trees
library(ada)
library(randomForest)
library(e1071)

#library(lubridate) #for handling dates
#library(car) ## for VIF
```


# Exploratory Data Analysis

## Reading in Data

```{r}
setwd("D:/INSOFE/Module 3 - ML/MITH")

train = read.csv("train.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
test = read.csv("test.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
valid = read.csv("valid.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)

```


## Data Summary

```{r}
dim(train); str (train); #View(train)
dim(test); str (test); #View(test)
```


## Change attributes to respective types

### Bind the data together in a single data frame

```{r}

### Bind together the data

data = rbind(train,valid)
test$Price = 0
data = rbind(data,test)

names(data)
```

### Check the String attributes from the data description

```{r}
# NameOfTheVehicle : Firstly i tried to do string replacement and substitution to extract car name, but failed to do so, hence decided to not include this column
# ModelOfTheVehicle : change to factors, and later combine those levels where frequency is less than 3%

length(unique(data$NameOfTheVehicle))
length(unique(data$ModelOfTheVehicle))
data$VehicleName = gsub("[0-9]|\\*+|%+|#+|!+|\\.+|\\\\+|/+|\\?+|>+|\\++|~+","", data$NameOfTheVehicle)
data$VehicleName = gsub("_+","_", data$VehicleName)
data$VehicleName = gsub("^_","", data$VehicleName)

car = c("audi","mercedes","ford","cabrio","cdi","mazda","klasse","daimler","limousine","limusine","bmw","bwm","renault","clio","carisma","biete","berlingo", "sportback", "sport","caddy","chevrolet","auto","alfa","astra", "chrysler","citroën","citroeen","dacia","citroen","paket","duster","daihatsu","xenon","corsa","daewo","fiat","gepflegter","golf","fiesta","erhaltenen","civic", "accord","guenstig","honda","hyundai","guter","jaguar", "volvo", "cherokee" , "kia", "jahren","klima","Kleiner","lada","laguna","lancia","rover","lupo","nissan","skoda","volkswagen","peugeot","subaru","opel","mitsubishi","toyota","porsche","ibiza","suzuki","nissan","cooper","touran","polo","wrangler","highline","tiguan","leon","passat")

for(i in car){
  data$VehicleName = ifelse(grepl(i,tolower(data$VehicleName) ),i,data$VehicleName)
}



#prop.table(table(data$VehicleName))

#unique(data$VehicleName)
data$NameOfTheVehicle = NULL


```

### The date attributes

```{r}
## Datecols

#For DataCollectedDate, DateOfAdCreation and DateOfAdLastSeen, I will extract month,year and month respectively
# 
# data$DataCollectedDate = as.Date.character(data$DataCollectedDate,'%d-%m-%Y')
# data$DataCollectedDate = format(data$DataCollectedDate,'%m')
#  
# data$DateOfAdCreation = as.Date.character(data$DateOfAdCreation,'%d-%m-%Y')
# data$DateOfAdCreation = format(data$DateOfAdCreation,'%Y')
#  
# data$DateOfAdLastSeen = as.Date.character(data$DateOfAdLastSeen,'%d-%m-%Y')
# data$DateOfAdLastSeen = format(data$DateOfAdLastSeen,'%m')

## Remove these columns

data$DataCollectedDate = NULL
data$DateOfAdCreation = NULL
data$DateOfAdLastSeen = NULL
str(data)
```

### Removing other columns which dont add any meaning

```{r}

data$VehicleID = NULL
data$ZipCode = NULL


## Check for Near Zero Variance
nzv <- nearZeroVar(data) #NumberOfPictures,SellerType,OfferType remove this column also
data$NumberOfPictures = NULL
data$SellerType = NULL
data$OfferType = NULL
data$MonthOfVehicleRegistration= NULL

str(data)


```

### Categorical and Numeric Attributes

```{r}


factCols = c("VehicleType","YearOfVehicleRegistration","GearBoxType","ModelOfTheVehicle","TypeOfTheFuelUsed","BrandOfTheVehicle","IsDamageRepaired","VehicleName")
numCols = c("Price","PowerOfTheEngine","DistranceTravelled")


## Categorical

data[,factCols] <- data.frame(apply(data[,names(data) %in% factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))

## Numeric

data[,numCols] <- data.frame(apply(data[,names(data) %in% numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))

str(data)
```



### Separate numerical and categorical data for further analysis

```{r}

num_data = data[,names(data) %in% numCols]
cat_data = data[,names(data) %in% factCols]

str(num_data)
str(cat_data)
```

## Plotting the data

```{r}


ggplot(data, aes(x=VehicleName, y=Price)) + stat_summary(fun.y="mean", geom="bar")

VehicleName.price = data.frame(aggregate(Price ~ VehicleName,data = data, FUN = mean ))

VehicleName.price[-order(VehicleName.price$Price),]


```



```{r}

## It will be worthwhile to combine the levels which have very less frequency

#Plotting the String attributes
barchart(data$ModelOfTheVehicle)



```

## Plots of numerical attributes vs Target Attribute
```{r}

## Most of the Engine Powers associated with higher power doesnt have hightest price

boxplot(data$PowerOfTheEngine) ## Too many outliers in this attribute, removing these outliers did not do siginificant improvement to model as I used Tree based models, and they are resistant to outliers.

ggplot(data=data)+
  geom_point(aes(x = PowerOfTheEngine, y=Price)) + 
  scale_y_continuous("Price of car", breaks = seq(0,100000,1000))


```


```{r}

## Price is distributed proportionally across all distance travelled values, my assumption that price would be less with distance travelled is not correct

ggplot(data=data)+
  geom_point(aes(x = DistranceTravelled, y=Price)) + 
  scale_y_continuous("Price of car", breaks = seq(0,100000,1000))
```

```{r}

## The older year of registration, have lesser prices

ggplot(data=data)+
  geom_point(aes(x = YearOfVehicleRegistration, y=Price)) + 
  scale_y_continuous("Price of car", breaks = seq(0,100000,1000))
```


```{r}

## Even though "Yes" are less, still decided not to drop this attribute

ggplot(data=data) +
  geom_point(mapping=aes(x = Price, y = PowerOfTheEngine, color = VehicleName))

```

# Data Cleaning

## Numeric Variables

```{r}
# Missing numeric variables


sum(is.na(num_data))

# Correlations

numCorr <- findCorrelation(cor(num_data), cutoff = .80) # None found
#num_train <- num_train[,-numCorr] 
#num_test <- num_test[,-numCorr]


```

## Categorical variables

```{r}
# sum(is.na(cat_train))
# sum(is.na(cat_test))
# sum(is.na(cat_valid))

sum(is.na(cat_data))

## percentage of missing values per column


missing.cat.data = sapply(cat_data, function(x) {(sum(is.na(x))/length(x))*100})

## For remaining missing values, label as "unavailable"

#Convert to character

cat_data <- data.frame(apply(cat_data,MARGIN = 2,FUN = function(x) as.character(x)))

#Set missing value to Unavailable, i = row, j = col


for (i in seq_along(cat_data)) set(cat_data, i=which(is.na(cat_data[[i]])), j=i, value="Unavailable")

#Convert back to factors

cat_data <- data.frame(apply(cat_data,MARGIN = 2,FUN = function(x) as.factor(as.character(x))))

```

# Data Manipulation
* Combine categorical variables which have several levels with low frequencies.
```{r}
#combine factor levels with less than 3% values

for(i in names(cat_data)){
                  p <- 2/100
                  ld <- names(which(prop.table(table(cat_data[[i]])) < p))
                  levels(cat_data[[i]])[levels(cat_data[[i]]) %in% ld] <- "Other"
}


# Hygiene check
#check columns with unequal levels (library mlr)

summarizeColumns(cat_data)[,"nlevs"]

str(cat_data)
```


# Machine Learning

## Create the data
```{r}

# Standardize the data

preProc<-preProcess(num_data[,-1],method = c("center", "scale"))
num_data[,-1] <- predict(preProc,num_data[,-1])


#combine num and cat data

data_combi = cbind(num_data,cat_data)

# Retrieve the Test data back

d_test = data_combi[78467:90438,]

# Remove the test data from combined data

data_combi = data_combi[-c(78467:90438),]

#Split into train and valid

library(caTools)

set.seed(1000)
split = sample.split(data_combi$Price, SplitRatio = 0.70)

d_train = subset(data_combi, split==TRUE)

d_valid = subset(data_combi, split==FALSE)





```


## Trying different Algorithms

### Preparing the data in required format

```{r}


target_train = d_train$Price
target_valid = d_valid$Price

d_train <- as.data.frame(model.matrix(Price~.-1,data=d_train))
d_test <- as.data.frame(model.matrix(Price~.-1,data=d_test))
d_valid <- as.data.frame(model.matrix(Price~.-1,data=d_valid))

# Blank space in names of columns was throwing error, while running RF, so changed blank to underscore
names(d_train) = gsub(" ","_",names(d_train))
names(d_test) = gsub(" ","_",names(d_test))
names(d_valid) = gsub(" ","_",names(d_valid))

# Assigning back the Target variable
d_train$Price = target_train
d_test$Price = 0
d_valid$Price = target_valid

str(d_train)

```

### Model1 : Linear Regression (For getting basic understanding) and benchmarking

```{r}

set.seed(26)
fit.linear1 = lm(Price~.,data=d_train)
summary(fit.linear1)

####Adjusted R-squared:  0.68

plot(fit.linear1)

## The assumptions of linearity are not held true, there is heteroscedasticity, influentual observations, and funnel shaped pattern

## Update the model based on cooks distance and studentized residual

w <- abs(rstudent(fit.linear1)) < 3 & abs(cooks.distance(fit.linear1)) < 4/nrow(fit.linear1$model)

fit.linear2 <- update(fit.linear1, weights=as.numeric(w))

summary(fit.linear2)

plot(fit.linear2)

#####Adjusted R-squared:  0.7944

```

## Plot the target variable to see distribution
```{r}

## Distribution is right skewed
qplot(x = Price, data = d_train)

## Take logarithm of Price to make it close to Gaussian distribution
qplot(x = log(Price), data = d_train)


```

### Linear Regression on transformed variable

```{r}

d_train_t = d_train
d_train_t$Price = log(d_train_t$Price)

d_valid_t = d_valid
d_valid_t$Price = log(d_valid_t$Price)


fit.linear3 <- lm(Price~. , data=d_train_t)

summary(fit.linear3)

# Adjusted R-squared:  0.7764

plot(fit.linear3)

## Update the model based on cooks distance and studentized residual

w1 <- abs(rstudent(fit.linear3)) < 3 & abs(cooks.distance(fit.linear3)) < 4/nrow(fit.linear3$model)

fit.linear4 <- update(fit.linear3, weights=as.numeric(w1))

summary(fit.linear4)

plot(fit.linear4)

# Adjusted R-squared:  0.8673

## Metrics on updated model

## Error on train data

regr.eval(fit.linear4$fitted.values,d_train_t$Price) 

## Error on Valid dat

fit.linear.validpred = predict(fit.linear4,d_valid_t)
regr.eval(fit.linear.validpred,d_valid_t$Price) 

##### Around 0.03 on Train and Validation Set

## Test data predictions

fit.linear.testpred = exp(predict(fit.linear4,d_test))

submissions = data.frame("VehicleID" = test$VehicleID,"Price" = fit.linear.testpred)
write.csv(x = submissions,file="prediction.csv",row.names = FALSE)

```



## Build another models on transformed data

### Model2 : CART for regression

```{r}

fileName = "fit.CART.model.RData"

if(file.exists(fileName)) {
  
  load(fileName)
  
} else {
  
  ## From the complexity table, tune the cp parameter, and prune the tree
  fit.CART <- rpart(Price ~ ., d_train_t,control = rpart.control(cp = 0.0021))
  save(fit.CART,file = fileName)
  
}


plotcp(fit.CART)

plot(fit.CART);text(fit.CART)

#summary(fit.CART)

## On Train Data

cart.train.pred = predict(fit.CART,d_train_t)

regr.eval(d_train_t$Price,cart.train.pred)

## On Valid Data

cart.valid.pred = predict(fit.CART,d_valid_t)

regr.eval(d_valid_t$Price,cart.valid.pred)

## Test data predictions

cart.test.pred = exp(predict(fit.CART,d_test))

submissions = data.frame("VehicleID" = test$VehicleID,"Price" = cart.test.pred)
write.csv(x = submissions,file="prediction.csv",row.names = FALSE)

```

### Model 3 : Random Forest

```{r}

fileName = "fit.rf.model.RData"

if(file.exists(fileName)) {
  
  load(fileName)
} else {
  
  fit.rf = randomForest(Price ~ ., data=d_train_t, keep.forest=TRUE, ntree=50, mtry = 40,do.trace = TRUE)
  save(fit.rf,file = fileName)
  
}

  plot(fit.rf)
  summary(fit.rf)
  
  
  ## On Train Data
  
  rf.train.pred = predict(fit.rf,d_train_t)
  
  regr.eval(d_train_t$Price,rf.train.pred)
  
  ## On Valid Data
  
  rf.valid.pred = predict(fit.rf,d_valid_t)
  
  regr.eval(d_valid_t$Price,rf.valid.pred)
  
  ## Test data predictions
  
  rf.test.pred = exp(predict(fit.rf,d_test))
  
  submissions = data.frame("VehicleID" = test$VehicleID,"Price" = rf.test.pred)
  write.csv(x = submissions,file="prediction.csv",row.names = FALSE)



```

### Model 4 : Xtreme Gradient Boosting
* Tuning Parameters

nrounds (# Boosting Iterations)
max_depth (Max Tree Depth)
eta (Shrinkage)
gamma (Minimum Loss Reduction)
colsample_bytree (Subsample Ratio of Columns)
min_child_weight (Minimum Sum of Instance Weight)
subsample (Subsample Percentage)



```{r}

fileName = "fit.xgb.model.RData"

if(file.exists(fileName)) {
  
  load(fileName)
} else {
  
  
## Building using random search

ctrl <- caret::trainControl(method = "repeatedcv",
                     number = 5,
                     allowParallel = TRUE)


xgb.grid <- expand.grid(nrounds = c(700,500), #the maximum number of iterations
                        eta = c(0.03,0.01), # shrinkage
                        max_depth = c(5,7),
                        subsample = 0.8,
                        gamma = c(30,50),               #default=0
                        colsample_bytree = 0.8,    #default=1
                        min_child_weight = 1)     #default=1)

fit.xgb <- caret::train(x=d_train_t[,-44],
                 y=d_train_t$Price,
                 method="xgbTree",
                 metric="RMSE",
                 trControl=ctrl,
                 tuneGrid=xgb.grid,
                 verbose = TRUE)

save(fit.xgb,file = fileName)

}

fit.xgb$bestTune
plot(fit.xgb)

xgbImp <- varImp(fit.xgb, scale = FALSE)
plot(xgbImp)


## On Train Data

xgb.train.pred = predict(fit.xgb,d_train_t)

regr.eval(d_train_t$Price,xgb.train.pred)

## On Valid Data

xgb.valid.pred = predict(fit.xgb,d_valid_t)

regr.eval(d_valid_t$Price,xgb.valid.pred)

## Test data predictions

xgb.test.pred = exp(predict(fit.xgb,d_test))

submissions = data.frame("VehicleID" = test$VehicleID,"Price" = xgb.test.pred)
write.csv(x = submissions,file="prediction.csv",row.names = FALSE)
                
```

# Build Ensemble model using STACKING

## Create the data frame using predicitons from our previous models

```{r}


train_Pred_All_Models = data.frame(RF = exp(rf.train.pred),
                                   XGBTREE = exp(xgb.train.pred),
                                   CART = exp(cart.train.pred),
                                   LM = exp(fit.linear4$fitted.values))

train_Pred_All_Models$Price = d_train$Price

valid_Pred_All_Models = data.frame(RF = exp(rf.valid.pred),
                                   XGBTREE = exp(xgb.valid.pred),
                                   CART = exp(cart.valid.pred),
                                   LM = exp(fit.linear.validpred))

valid_Pred_All_Models$Price = d_valid$Price

test_Pred_All_Models = data.frame(RF = rf.test.pred,
                                   XGBTREE = xgb.test.pred,
                                  CART = cart.test.pred,
                                  LM = fit.linear.testpred)

test_Pred_All_Models$Price = d_test$Price ## Zero Values only
head(train_Pred_All_Models)

## Check Correlations :All are highly correlated, donot expect much improvemnt in models

cor(train_Pred_All_Models)


```

## Ensemble model with CART as meta learner
```{r}


fileName = "fit.CART.ensemble.model.RData"

if(file.exists(fileName)) {
  
  load(fileName)
} else {
  

fit.CART.ensemble <- rpart(Price ~ ., train_Pred_All_Models,control = rpart.control(cp = 0.012))

#printcp(fit.CART.ensemble)

#plotcp(fit.CART.ensemble)

save(fit.CART.ensemble,file = fileName)

}

#summary(fit.CART.ensemble)
## On Train Data

CART.ensemble.train.pred = predict(fit.CART.ensemble,train_Pred_All_Models)

regr.eval(train_Pred_All_Models$Price,CART.ensemble.train.pred)

## On Valid Data

CART.ensemble.valid.pred = predict(fit.CART.ensemble,valid_Pred_All_Models)

regr.eval(valid_Pred_All_Models$Price,CART.ensemble.valid.pred)

## Test data predictions

CART.ensemble.test.pred = predict(fit.CART.ensemble,test_Pred_All_Models)

submissions = data.frame("VehicleID" = test$VehicleID,"Price" = CART.ensemble.test.pred)
write.csv(x = submissions,file="prediction.csv",row.names = FALSE)



```

## Ensemble model with Linear Regression as meta learner

```{r}

fit.lm.ensemble = lm(Price ~ ., data=train_Pred_All_Models) 
summary(fit.lm.ensemble)
plot(fit.lm.ensemble)


## On Train Data

lm.ensemble.train.pred = predict(fit.lm.ensemble,train_Pred_All_Models)

regr.eval(train_Pred_All_Models$Price,lm.ensemble.train.pred)

## On Valid Data

lm.ensemble.valid.pred = predict(fit.lm.ensemble,valid_Pred_All_Models)

regr.eval(valid_Pred_All_Models$Price,lm.ensemble.valid.pred)

## Test data predictions

lm.ensemble.test.pred = predict(fit.lm.ensemble,test_Pred_All_Models)

submissions = data.frame("VehicleID" = test$VehicleID,"Price" = lm.ensemble.test.pred)
write.csv(x = submissions,file="prediction.csv",row.names = FALSE)

```

## Fit using RF as meta learner

```{r}

fileName = "fit.rf.ensemble.model.RData"

if(file.exists(fileName)) {
  
  load(fileName)
  
} else {
  
  fit.rf.ensemble = randomForest(Price ~ ., data=train_Pred_All_Models,keep.forest=TRUE, ntree=3) 
  save(fit.rf.ensemble,file = fileName)
  
}


# Print and understand the model
print(fit.rf.ensemble)

plot(fit.rf.ensemble)

# Important attributes
fit.rf.ensemble$importance  
round(importance(fit.rf.ensemble), 2)


## On Train Data

rf.ensemble.train.pred = predict(fit.rf.ensemble,train_Pred_All_Models)

regr.eval(train_Pred_All_Models$Price,rf.ensemble.train.pred)

## On Valid Data

rf.ensemble.valid.pred = predict(fit.rf.ensemble,valid_Pred_All_Models)

regr.eval(valid_Pred_All_Models$Price,rf.ensemble.valid.pred)

## Test data predictions

rf.ensemble.test.pred = predict(fit.rf.ensemble,test_Pred_All_Models)

submissions = data.frame("VehicleID" = test$VehicleID,"Price" = rf.ensemble.test.pred)
write.csv(x = submissions,file="prediction.csv",row.names = FALSE)

```

