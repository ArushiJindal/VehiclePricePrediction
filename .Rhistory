# eval.metric allows us to monitor two new metrics for each round, logloss and error.
importance <- xgb.importance(feature_names = setdiff(names(d_train),"income_level"), model = model)
print(importance)
xgb.plot.importance(importance_matrix = importance)
prob_train <- predict(model, as.matrix(d_train[,setdiff(names(d_train),"income_level")]))
prob_train
prob_train = ifelse(prob_train>0.5,1,0)
confusionMatrix(prob_train,d_train$income_level)
model = xgb.train(data=dmatrix_train, max.depth=10,
eta=0.3, nthread = 5, nround=150,
watchlist=watchlist,
eval.metric = "error",
objective = "binary:logistic",
verbose=1)
prob_train = ifelse(prob_train>0.5,1,0)
confusionMatrix(prob_train,d_train$income_level)
prob_train <- predict(model, as.matrix(d_train[,setdiff(names(d_train),"income_level")]))
pred_test <- predict(model, as.matrix(d_test[,setdiff(names(d_test),"income_level")]))
prob_train = ifelse(prob_train>0.5,1,0)
confusionMatrix(prob_train,d_train$income_level)
confusionMatrix(pred_test,d_test$income_level)
pred_test = ifelse(pred_test>0.5,1,0)
confusionMatrix(pred_test,d_test$income_level)
model = xgb.train(data=dmatrix_train, max.depth=10,
eta=0.3, nthread = 5, nround=300,
watchlist=watchlist,
eval.metric = "error",
objective = "binary:logistic",
verbose=1)
# eval.metric allows us to monitor two new metrics for each round, logloss and error.
importance <- xgb.importance(feature_names = setdiff(names(d_train),"income_level"), model = model)
print(importance)
xgb.plot.importance(importance_matrix = importance)
prob_train <- predict(model, as.matrix(d_train[,setdiff(names(d_train),"income_level")]))
prob_train <- predict(model, as.matrix(d_train[,setdiff(names(d_train),"income_level")]))
pred_test <- predict(model, as.matrix(d_test[,setdiff(names(d_test),"income_level")]))
prob_train <- predict(model, as.matrix(d_train[,setdiff(names(d_train),"income_level")]))
pred_test <- predict(model, as.matrix(d_test[,setdiff(names(d_test),"income_level")]))
library(ROCR)
pred_test <- predict(model, as.matrix(d_test[,setdiff(names(d_test),"income_level")]))
library(ROCR)
ROCRpred = prediction(prob_train, d_train$income_level)
library(ROCR)
ROCRpred = prediction(prob_train, d_train$income_level)
as.numeric(performance(ROCRpred, "auc")@y.values)
as.numeric(performance(ROCRpred, "auc")@y.values)
ROCRperf <- performance(ROCRpred, "tpr", "fpr")
par(mfrow=c(1,1))
plot(ROCRperf,,col=rainbow(10), colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
plot(ROCRperf,col=rainbow(10), colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
prob_train = ifelse(prob_train>0.2,1,0)
pred_test = ifelse(pred_test>0.2,1,0)
confusionMatrix(prob_train,d_train$income_level)
confusionMatrix(pred_test,d_test$income_level)
library(parallel)
library(doParallel)
detectCores()
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls(all=TRUE))
library(caret)
library(dummies)
library(DMwR)
library(infotheo)
library(caretEnsemble)
library(xgboost)
library(ggplot2)
library(plotly)
library(gbm)
library(data.table) #To be able to change multiple columns in one go
library(mlr) #for machine learning
setwd("D:/INSOFE/Module 3 - ML/XGBoost_Arushi/Census Data Practise")
train = read.csv("train.csv",header=TRUE,na.strings = c(""," ","?","NA",NA))
test = read.csv("test.csv",header=TRUE,na.strings = c(""," ","?","NA",NA))
dim(train); str (train); View(train)
dim(test); str (test); View(test)
## Check head
head(train)
## Check the Target Variable
unique(train$income_level)
unique(test$income_level)
## Since the Target variables are different, encode them to 0 or 1
train$income_level = ifelse(train$income_level == -50000,0,1)
test$income_level = ifelse(test$income_level == -50000,0,1)
## Severity of class imbalance
round(prop.table(table(train$income_level))*100) # ==> implies getting 94% accuracy is very easy, we need to focus on other metrics to be able to predict the minority class correctly
factCols <- c(2:5,7,8:16,20:29,31:38,40,41)
numCols = setdiff(1:41,factCols)
## Categorical
train[,factCols] <- data.frame(apply(train[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
test[,factCols] <- data.frame(apply(test[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
str(train[,factCols])
## Numeric
train[,numCols] <- data.frame(apply(train[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
test[,numCols] <- data.frame(apply(test[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
str(train[,numCols])
## Separate numerical and categorical data for further analysis
num_train = train[,numCols]
num_test = test[,numCols]
cat_train = train[,factCols]
cat_test = test[,factCols]
## Remove so that memory is available for other computations
rm(test,train)
## Generic Plot Function
tr = function(a) {
ggplot(data = num_train, aes(x = a,y = ..density..)) +
geom_histogram(fill = "blue", color = "red", alpha = 0.5, bins = 100) + geom_density()
}
tr(num_train$age)
## if we think of the problem we are trying to solve, population below age 20 could not earn >50K under normal circumstances? Therefore, we can bin this variable into age groups.
## Very much right skew in this attribute
tr(num_train$capital_losses)
tr(num_train$num_person_Worked_employer)
num_train$income_level = cat_train$income_level
ggplot(data=num_train)+
geom_point(aes(x = age, y=wage_per_hour,colour=income_level))+
scale_y_continuous("wage per hour", breaks = seq(0,10000,1000))
## most of the people having income_level 1, seem to fall in the age of 25-65 earning wage of $1000 to $4000 per hour. This plot further strengthens our assumption that age < 20 would have income_level 0, hence we will bin this variable.
all_bar <- function(i){
ggplot(data = cat_train)+
geom_bar(aes(x=i,fill=income_level),position = "dodge",  color="black")+
scale_fill_brewer(palette = "Pastel1")+
theme(axis.text.x =element_text(angle  = 60,hjust = 1,size=10))
}
all_bar(cat_train$class_of_worker)
## a good practice is to combine levels having less than 5% frequency of the total category frequency.
all_bar(cat_train$education)
## all children have income_level 0. Also, we can infer than Bachelors degree holders have the largest proportion of people have income_level 1.
# Missing numeric variables
num_train$income_level = NULL ## Remove it from numeric
sum(is.na(num_train))
sum(is.na(num_test))
# Correlations
numCorr <- findCorrelation(cor(num_train), cutoff = .70)
num_train <- num_train[,-numCorr]
num_test <- num_test[,-numCorr]
sum(is.na(cat_train))
sum(is.na(cat_test))
## percentage of missing values per column
missing.cat.train = sapply(cat_train, function(x) {(sum(is.na(x))/length(x))*100}) #Some variables have approximately 50% data missing, let us remove these columns
missing.cat.test = sapply(cat_test, function(x) {(sum(is.na(x))/length(x))*100})
## Keep columns where missing values are less than 5% only
cat_train = subset(cat_train,select = missing.cat.train < 5)
cat_test <- subset(cat_test, select = missing.cat.test < 5)
## For remaining missing values, label as "unavailable"
#Convert to character
cat_train <- data.frame(apply(cat_train,MARGIN = 2,FUN = function(x) as.character(x)))
cat_test <- data.frame(apply(cat_test,MARGIN = 2,FUN = function(x) as.character(x)))
#Set missing value to Unavailable, i = row, j = col
for (i in seq_along(cat_train)) set(cat_train, i=which(is.na(cat_train[[i]])), j=i, value="Unavailable")
for (i in seq_along(cat_test)) set(cat_test, i=which(is.na(cat_test[[i]])), j=i, value="Unavailable")
#Convert back to factors
cat_train <- data.frame(apply(cat_train,MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
cat_test <- data.frame(apply(cat_test,MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
#combine factor levels with less than 5% values
for(i in names(cat_train)){
p <- 5/100
ld <- names(which(prop.table(table(cat_train[[i]])) < p))
levels(cat_train[[i]])[levels(cat_train[[i]]) %in% ld] <- "Other"
}
for(i in names(cat_test)){
p <- 5/100
ld <- names(which(prop.table(table(cat_test[[i]])) < p))
levels(cat_test[[i]])[levels(cat_test[[i]]) %in% ld] <- "Other"
}
# Hygiene check
#check columns with unequal levels (library mlr)
summarizeColumns(cat_train)[,"nlevs"]
summarizeColumns(cat_test)[,"nlevs"]
#bin age variable 0-30 31-60 61 - 90
num_train$age = as.factor(cut(x = num_train$age,breaks = c(0,30,60,90),include.lowest = TRUE,labels = c("young","adult","old")))
num_test$age = as.factor(cut(x = num_test$age,breaks = c(0,30,60,90),include.lowest = TRUE,labels = c("young","adult","old")))
str(num_train$age)
str(num_test$age)
num_train$wage_per_hour = ifelse(num_train$wage_per_hour == 0,"Zero","MoreThanZero")
num_train$wage_per_hour = as.factor(as.character(num_train$wage_per_hour))
num_train$capital_gains = ifelse(num_train$capital_gains == 0,"Zero","MoreThanZero")
num_train$capital_gains = as.factor(as.character(num_train$capital_gains))
num_train$capital_losses = ifelse(num_train$capital_losses == 0,"Zero","MoreThanZero")
num_train$capital_losses = as.factor(as.character(num_train$capital_losses))
num_train$dividend_from_Stocks = ifelse(num_train$dividend_from_Stocks == 0,"Zero","MoreThanZero")
num_train$dividend_from_Stocks = as.factor(as.character(num_train$dividend_from_Stocks))
num_test$wage_per_hour = ifelse(num_test$wage_per_hour == 0,"Zero","MoreThanZero")
num_test$wage_per_hour = as.factor(as.character(num_test$wage_per_hour))
num_test$capital_gains = ifelse(num_test$capital_gains == 0,"Zero","MoreThanZero")
num_test$capital_gains = as.factor(as.character(num_test$capital_gains))
num_test$capital_losses = ifelse(num_test$capital_losses == 0,"Zero","MoreThanZero")
num_test$capital_losses = as.factor(as.character(num_test$capital_losses))
num_test$dividend_from_Stocks = ifelse(num_test$dividend_from_Stocks == 0,"Zero","MoreThanZero")
num_test$dividend_from_Stocks = as.factor(as.character(num_test$dividend_from_Stocks))
#combine num and cat data
d_train = cbind(num_train,cat_train)
d_test = cbind(num_test,cat_test)
rm(num_train,num_test,cat_train,cat_test) #save memory
setdiff(names(d_test),names(d_train))
d_test_copy = d_test
d_test$migration_msa = NULL
d_test$migration_reg = NULL
d_test$migration_within_reg = NULL
d_test$migration_sunbelt = NULL
str(d_train)
#create a task
train.task <- makeClassifTask(data = d_train,target = "income_level")
test.task <- makeClassifTask(data= d_test,target = "income_level")
#remove zero variance features
train.task <- removeConstantFeatures(train.task)
test.task <- removeConstantFeatures(test.task)
#get variable importance chart
var_imp <- generateFilterValuesData(train.task, method = c("information.gain"))
plotFilterValues(var_imp,feat.type.cols = TRUE)
target_train = d_train$income_level
target_test = d_test$income_level
d_train <- as.data.frame(model.matrix(income_level~.-1,data=d_train))
d_test <- as.data.frame(model.matrix(income_level~.-1,data=d_test))
d_train$income_level = target_train
d_test$income_level = target_test
str(d_train)
d_train$income_level <- make.names(as.factor(as.character(d_train$income_level)))
d_test$income_level <- make.names(as.factor(as.character(d_test$income_level)))
getModelInfo()$rpart$type
library(caret)
library(xgboost)
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
ctrl <- trainControl(method = "repeatedcv",   # n fold cross validation
number = 10,							# do 5 repititions of cv
summaryFunction=twoClassSummary,	# Use AUC to pick the best model
classProbs=TRUE,
allowParallel = TRUE)
adaboost.grid <- expand.grid(nIter  = c(2,5), # Maximum Number of Leaves
method  = c(1)) # Number of Trees
adaboost.tune <-caret::train(x=d_train[,-73],y=d_train$income_level,
data = d_train,
method="adaboost",
metric="ROC",
trControl=ctrl,
tuneGrid=NULL,
search = "random")
getModelInfo()$rpart$type
library(caret)
library(xgboost)
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
ctrl <- trainControl(method = "repeatedcv",   # n fold cross validation
number = 10,							# do 5 repititions of cv
summaryFunction=twoClassSummary,	# Use AUC to pick the best model
classProbs=TRUE,
allowParallel = TRUE)
adaboost.grid <- expand.grid(nIter  = c(2,5), # Maximum Number of Leaves
method  = c(1)) # Number of Trees
adaboost.tune <-caret::train(x=d_train[,-73],
y=d_train$income_level,
method="adaboost",
metric="ROC",
trControl=ctrl,
tuneGrid=NULL,
search = "random")
setwd("D:/INSOFE/Module 3 - ML/MITH")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls(all=TRUE))
library(caret)
library(dummies)
library(DMwR)
library(infotheo)
library(caretEnsemble)
library(xgboost)
library(ggplot2)
library(plotly)
library(gbm)
library(data.table) #To be able to change multiple columns in one go
library(mlr) #for machine learning
setwd("D:/INSOFE/Module 3 - ML/MITH")
train = read.csv("train.csv",header=TRUE,na.strings = c(""," ","?","NA",NA))
test = read.csv("test.csv",header=TRUE,na.strings = c(""," ","?","NA",NA))
dim(train); str (train); View(train)
dim(test); str (test); View(test)
dim(train); str (train); #View(train)
dim(test); str (test); #View(test)
head(train)
unique(train$NameOfTheVehicle)
table(unique(train$NameOfTheVehicle))
len(unique(train$NameOfTheVehicle))
length(unique(train$NameOfTheVehicle))
length(unique(train$ModelOfTheVehicle))
## Categorical
train[,factCols] <- data.frame(apply(train[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
# Categorical and Numeric Attributes
factCols = c(4:5,7,9,13:16)
numCols = c(6,8,10,19)
## Categorical
train[,factCols] <- data.frame(apply(train[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
test[,factCols] <- data.frame(apply(test[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
str(train[,factCols])
## Numeric
train[,numCols] <- data.frame(apply(train[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
test[,numCols] <- data.frame(apply(test[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
str(train[,numCols])
str(train[,numCols])
str(train[,factCols])
dateCols = c(2,17,20)
str(train[,dateCols])
library(lubridate) #for handling dates
train[,dateCols] <- data.frame(apply(train[,dateCols],MARGIN = 2,FUN = function(x) dmy(x)))
test[,dateCols] <- data.frame(apply(test[,dateCols],MARGIN = 2,FUN = function(x) dmy(x)))
str(train[,dateCols])
setwd("D:/INSOFE/Module 3 - ML/MITH")
train = read.csv("train.csv",header=TRUE,na.strings = c(""," ","?","NA",NA))
test = read.csv("test.csv",header=TRUE,na.strings = c(""," ","?","NA",NA))
length(unique(train$NameOfTheVehicle))
length(unique(train$ModelOfTheVehicle))
# Categorical and Numeric Attributes
factCols = c(4:5,7,9,13:16)
numCols = c(6,8,10,19)
dateCols = c(2,17,20)
## Categorical
train[,factCols] <- data.frame(apply(train[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
test[,factCols] <- data.frame(apply(test[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
str(train[,factCols])
## Numeric
train[,numCols] <- data.frame(apply(train[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
test[,numCols] <- data.frame(apply(test[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
str(train[,numCols])
str(train[,dateCols])
setwd("D:/INSOFE/Module 3 - ML/MITH")
train = read.csv("train.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
test = read.csv("test.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
dim(train); str (train); #View(train)
dim(test); str (test); #View(test)
## Check the String attributes from the data description
length(unique(train$NameOfTheVehicle))
length(unique(train$ModelOfTheVehicle))
# Categorical and Numeric Attributes
factCols = c(4:5,7,9,13:16)
numCols = c(6,8,10,19)
dateCols = c(2,17,20)
## Categorical
train[,factCols] <- data.frame(apply(train[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
test[,factCols] <- data.frame(apply(test[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
str(train[,factCols])
## Numeric
train[,numCols] <- data.frame(apply(train[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
test[,numCols] <- data.frame(apply(test[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
str(train[,numCols])
str(train[,dateCols])
train[,dateCols] <- data.frame(apply(train[,dateCols],MARGIN = 2,FUN = function(x) dmy(x)))
test[,dateCols] <- data.frame(apply(test[,dateCols],MARGIN = 2,FUN = function(x) dmy(x)))
str(train[,dateCols])
setwd("D:/INSOFE/Module 3 - ML/MITH")
train = read.csv("train.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
test = read.csv("test.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
## Check the String attributes from the data description
length(unique(train$NameOfTheVehicle))
length(unique(train$ModelOfTheVehicle))
# Categorical and Numeric Attributes
factCols = c(4:5,7,9,13:16)
numCols = c(6,8,10,19)
dateCols = c(2,17,20)
## Categorical
train[,factCols] <- data.frame(apply(train[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
test[,factCols] <- data.frame(apply(test[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
str(train[,factCols])
## Numeric
train[,numCols] <- data.frame(apply(train[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
test[,numCols] <- data.frame(apply(test[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
str(train[,numCols])
str(train[,dateCols])
dmy("03-04-2016 11:54")
dmy_hm("03-04-2016 11:54")
str(train[,dateCols])
## Datecols, changing to format dmY, removing the time part, as we are not intersted in that much granularity
train[,dateCols] <- data.frame(apply(train[,dateCols],MARGIN = 2,FUN = function(x) dmy_hm(x)))
test[,dateCols] <- data.frame(apply(test[,dateCols],MARGIN = 2,FUN = function(x) dmy_hm(x)))
str(train[,dateCols])
setwd("D:/INSOFE/Module 3 - ML/MITH")
train = read.csv("train.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
test = read.csv("test.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
dim(train); str (train); #View(train)
dim(test); str (test); #View(test)
## Check the String attributes from the data description
length(unique(train$NameOfTheVehicle))
length(unique(train$ModelOfTheVehicle))
# Categorical and Numeric Attributes
factCols = c(4:5,7,9,13:16)
numCols = c(6,8,10,19)
dateCols = c(2,17,20)
## Categorical
train[,factCols] <- data.frame(apply(train[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
test[,factCols] <- data.frame(apply(test[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
str(train[,factCols])
## Numeric
train[,numCols] <- data.frame(apply(train[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
test[,numCols] <- data.frame(apply(test[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
str(train[,numCols])
str(train[,numCols])
str(train[,dateCols])
train[,dateCols] <- data.frame(apply(train[,dateCols],MARGIN = 2,FUN = function(x) as.Date(x)))
test[,dateCols] <- data.frame(apply(test[,dateCols],MARGIN = 2,FUN = function(x) as.Date(x)))
str(train[,dateCols])
setwd("D:/INSOFE/Module 3 - ML/MITH")
train = read.csv("train.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
test = read.csv("test.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
str(train[,dateCols])
as.Date("03-04-2016 11:54","%Y-%m-%d %H:%M:%S")
as.Date("03-04-2016 11:54","%Y-%m-%d %H:%M")
as.Date("03-04-2016 11:54","%d-%m-%Y %H:%M")
train[,dateCols] <- data.frame(apply(train[,dateCols],MARGIN = 2,FUN = function(x) as.Date(x,"%d-%m-%Y %H:%M")))
test[,dateCols] <- data.frame(apply(test[,dateCols],MARGIN = 2,FUN = function(x) as.Date(x,"%d-%m-%Y %H:%M")))
str(train[,dateCols])
dim(train); str (train); #View(train)
dim(test); str (test); #View(test)
## Check the String attributes from the data description
length(unique(train$NameOfTheVehicle))
length(unique(train$ModelOfTheVehicle))
# Categorical and Numeric Attributes
factCols = c(4:5,7,9,13:16)
numCols = c(6,8,10,19)
dateCols = c(2,17,20)
## Categorical
train[,factCols] <- data.frame(apply(train[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
test[,factCols] <- data.frame(apply(test[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
str(train[,factCols])
## Numeric
train[,numCols] <- data.frame(apply(train[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
test[,numCols] <- data.frame(apply(test[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
str(train[,numCols])
str(train)
tr = function(a) {
ggplot(data = train, aes(x = a,y = ..density..)) +
geom_histogram(fill = "blue", color = "red", alpha = 0.5, bins = 100) + geom_density()
}
tr(train$DataCollectedDate)
str(train)
tr(train$NameOfTheVehicle)
tr(train$PowerOfTheEngine)
table(unique(train$PowerOfTheEngine))
tr(train$YearOfVehicleRegistration)
tr(train$DateOfAdLastSeen)
setwd("D:/INSOFE/Module 3 - ML/MITH")
train = read.csv("train.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
test = read.csv("test.csv",header=TRUE,na.strings = c(""," ","?","NA",NA),stringsAsFactors = FALSE)
dim(train); str (train); #View(train)
dim(test); str (test); #View(test)
## Check the String attributes from the data description
length(unique(train$NameOfTheVehicle))
length(unique(train$ModelOfTheVehicle))
# Categorical and Numeric Attributes
factCols = c(4:5,7,9,13:16)
numCols = c(6,8,10,19)
dateCols = c(2,17,20)
## Categorical
train[,factCols] <- data.frame(apply(train[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
test[,factCols] <- data.frame(apply(test[,factCols],MARGIN = 2,FUN = function(x) as.factor(as.character(x))))
str(train[,factCols])
## Numeric
train[,numCols] <- data.frame(apply(train[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
test[,numCols] <- data.frame(apply(test[,numCols],MARGIN = 2,FUN = function(x) as.numeric(x)))
str(train[,numCols])
str(train)
tr(train$DateOfAdLastSeen)
length(unique(train$NameOfTheVehicle))
length(unique(train$ModelOfTheVehicle))
length(unique(train$ModelOfTheVehicle))
## Generic Plot Function
tr = function(a) {
ggplot(data = train, aes(x = a,y = ..count..)) +
geom_histogram(fill = "blue", color = "red", alpha = 0.5, bins = 100) + geom_count()
}
tr(train$DateOfAdLastSeen)
## Generic Plot Function
tr = function(a) {
ggplot(data = train, aes(x = a) +
stat_count(fill = "blue", color = "red", alpha = 0.5, bins = 100)
}
## Generic Plot Function
tr = function(a) {
ggplot(data = train, aes(x = a)) +
stat_count(fill = "blue", color = "red", alpha = 0.5, bins = 100)
}
tr(train$DateOfAdLastSeen)
## if we think of the problem we are trying to solve, population below age 20 could not earn >50K under normal circumstances? Therefore, we can bin this variable into age groups.
## Generic Plot Function
tr = function(a) {
ggplot(data = train, aes(x = a)) +
stat_count(fill = "blue", color = "red")
}
tr(train$DateOfAdLastSeen)
## if we think of the problem we are trying to solve, population below age 20 could not earn >50K under normal circumstances? Therefore, we can bin this variable into age groups.
str(train)
str(train[,dateCols])
str(train[,dateCols])
month <- format(as.Date(train$DataCollectedDate), "%m")
month
train$DataCollectedDate = NULL
train$DateOfAdCreation = NULL
train$DateOfAdLastSeen = NULL
test$DataCollectedDate = NULL
test$DateOfAdCreation = NULL
test$DateOfAdLastSeen = NULL
length(unique(train$VehicleID))
train$VehicleID = NULL
test$VehicleID = NULL
length(unique(train$ZipCode))
train$ZipCode = NULL
test$ZipCode = NULL
str(train)
str(train)
tr(train$NameOfTheVehicle)
